# CS698-Project
Two new activation functions for a neural network

This project explores two new activation functions which can be used in a neural network. One of the activation function resembles ReLU and the other activation function resembles a Sigmoid function. These functions were implemented using PyTorch.
